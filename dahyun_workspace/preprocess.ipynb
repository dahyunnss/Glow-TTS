{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "data_dict = {f\"seed_{i}\":{} for i in range(0, 10)}\n",
    "\n",
    "for seed in range(0, 10):\n",
    "    # 기존 구조: train_set: data['partition]['Train_Set], valid_set: data['partition']['Validation_Set']\n",
    "    # Test Set은 사용 x\n",
    "    # 모든 데이터 합친 후 6:2:2\n",
    "    file_list = list(data['partition']['Train_Set'].keys()) + list(data['partition']['Validation_Set'].keys())\n",
    "\n",
    "    train_len = int(len(file_list) * 0.6)\n",
    "    valid_len = int(len(file_list) * 0.2)\n",
    "    test_len = int(len(file_list) * 0.2)\n",
    "    \n",
    "    train_samples = random.sample(file_list, train_len) # 랜덤하게 뽑기\n",
    "    file_list = [file for file in file_list if not file in train_samples] # 안뽑힌 것들만 남기기\n",
    "    \n",
    "    valid_samples= random.sample(file_list, valid_len) # 랜덤하게 뽑기\n",
    "    file_list = [file for file in file_list if not file in valid_samples] # 안뽑힌 것들만 남기기\n",
    "    \n",
    "    test_samples = file_list # 나머진 테스트 셋\n",
    "    \n",
    "    seed_data_dict = {\"Train_Set\":train_samples,\n",
    "                      \"Validation_Set\":valid_samples,\n",
    "                      \"Test_Set\":test_samples\n",
    "                      }\n",
    "    \n",
    "    data_dict[f'seed_{seed}'] = seed_data_dict\n",
    "    \n",
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "# 파일 경로\n",
    "file_path = '/userHome/userhome2/dahyun/glow-tts/filelists/ljs_audio_text.txt'\n",
    "\n",
    "# 데이터 파일 읽기\n",
    "with open(file_path, 'r') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "# 함수: 데이터를 random seed를 사용하여 8:2로 나누고, 그 중 75%는 학습, 25%는 검증 데이터로 나눔\n",
    "def split_data(data, seed):\n",
    "    random.seed(seed)\n",
    "    random.shuffle(data)\n",
    "    \n",
    "    # 8:2 split (Train+Validation:Test)\n",
    "    split_index = int(len(data) * 0.8)\n",
    "    train_val_data = data[:split_index]\n",
    "    test_data = data[split_index:]\n",
    "    \n",
    "    # Train:Validation = 75:25 split\n",
    "    train_split_index = int(len(train_val_data) * 0.75)\n",
    "    train_data = train_val_data[:train_split_index]\n",
    "    validation_data = train_val_data[train_split_index:]\n",
    "    \n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "# 10개의 random seed에 따라 데이터를 나누어 저장할 딕셔너리\n",
    "split_results = {}\n",
    "\n",
    "# 10번 반복하여 seed 0~9로 데이터 분할\n",
    "for seed in range(10):\n",
    "    train_data, validation_data, test_data = split_data(data, seed)\n",
    "    split_results[f'seed_{seed}'] = {\n",
    "        'Train_Set': train_data,\n",
    "        'Validation_Set': validation_data,\n",
    "        'Test_Set': test_data\n",
    "    }\n",
    "    \n",
    "    # 각 시드별로 train, val, test 데이터 개수 출력\n",
    "    print(f\"Seed {seed}:\")\n",
    "    print(f\"  Train_Set: {len(train_data)} samples\")\n",
    "    print(f\"  Validation_Set: {len(validation_data)} samples\")\n",
    "    print(f\"  Test_Set: {len(test_data)} samples\")\n",
    "    print()\n",
    "\n",
    "# 결과를 json 파일로 저장\n",
    "output_file = '/userHome/userhome2/dahyun/glow-tts/filelists/glowtts_new_data_splits.json'\n",
    "with open(output_file, 'w') as json_file:\n",
    "    json.dump(split_results, json_file, indent=4)\n",
    "\n",
    "print(f\"Data splits saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glow_tts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
